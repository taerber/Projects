{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tara Erberich\n",
    "Student ID: 9237580532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.a\n",
    "\n",
    "Download the WDBC data from: https://archive.ics.uci.edu/ml/datasets/\n",
    "Breast+Cancer+Wisconsin+(Diagnostic). (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>0</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>0</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>0</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>926424</td>\n",
       "      <td>0</td>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>926682</td>\n",
       "      <td>0</td>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>926954</td>\n",
       "      <td>0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>927241</td>\n",
       "      <td>0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>92751</td>\n",
       "      <td>1</td>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0   1      2      3       4       5        6        7        8   \\\n",
       "0      842302   0  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1      842517   0  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2    84300903   0  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3    84348301   0  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    84358402   0  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..        ...  ..    ...    ...     ...     ...      ...      ...      ...   \n",
       "564    926424   0  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
       "565    926682   0  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
       "566    926954   0  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
       "567    927241   0  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
       "568     92751   1   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
       "\n",
       "          9   ...      22     23      24      25       26       27      28  \\\n",
       "0    0.14710  ...  25.380  17.33  184.60  2019.0  0.16220  0.66560  0.7119   \n",
       "1    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
       "2    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
       "3    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
       "4    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
       "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
       "564  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
       "565  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
       "566  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
       "567  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
       "568  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
       "\n",
       "         29      30       31  \n",
       "0    0.2654  0.4601  0.11890  \n",
       "1    0.1860  0.2750  0.08902  \n",
       "2    0.2430  0.3613  0.08758  \n",
       "3    0.2575  0.6638  0.17300  \n",
       "4    0.1625  0.2364  0.07678  \n",
       "..      ...     ...      ...  \n",
       "564  0.2216  0.2060  0.07115  \n",
       "565  0.1628  0.2572  0.06637  \n",
       "566  0.1418  0.2218  0.07820  \n",
       "567  0.2650  0.4087  0.12400  \n",
       "568  0.0000  0.2871  0.07039  \n",
       "\n",
       "[569 rows x 32 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import statsmodels.api as sm\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = pd.read_csv('Lab 3 Data/wdbc.data', header = None);\n",
    "#print(data[data.diagnosis == 'M'])\n",
    "\n",
    "data = data.replace('M', 0)\n",
    "data = data.replace('B', 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.b\n",
    "\n",
    "Choose the first 30 malignant cases and the first 50 benign cases in the data set\n",
    "as the test set and the rest as the training set. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         2      3       4       5        6        7        8        9   \\\n",
      "0    17.990  10.38  122.80  1001.0  0.11840  0.27760  0.30010  0.14710   \n",
      "1    20.570  17.77  132.90  1326.0  0.08474  0.07864  0.08690  0.07017   \n",
      "2    19.690  21.25  130.00  1203.0  0.10960  0.15990  0.19740  0.12790   \n",
      "3    11.420  20.38   77.58   386.1  0.14250  0.28390  0.24140  0.10520   \n",
      "4    20.290  14.34  135.10  1297.0  0.10030  0.13280  0.19800  0.10430   \n",
      "..      ...    ...     ...     ...      ...      ...      ...      ...   \n",
      "113  10.510  20.19   68.64   334.2  0.11220  0.13030  0.06476  0.03068   \n",
      "114   8.726  15.83   55.84   230.9  0.11500  0.08201  0.04132  0.01924   \n",
      "115  11.930  21.53   76.53   438.6  0.09768  0.07849  0.03328  0.02008   \n",
      "116   8.950  15.76   58.74   245.2  0.09462  0.12430  0.09263  0.02308   \n",
      "120  11.410  10.82   73.34   403.3  0.09373  0.06685  0.03512  0.02623   \n",
      "\n",
      "         10       11  ...      22     23      24      25      26      27  \\\n",
      "0    0.2419  0.07871  ...  25.380  17.33  184.60  2019.0  0.1622  0.6656   \n",
      "1    0.1812  0.05667  ...  24.990  23.41  158.80  1956.0  0.1238  0.1866   \n",
      "2    0.2069  0.05999  ...  23.570  25.53  152.50  1709.0  0.1444  0.4245   \n",
      "3    0.2597  0.09744  ...  14.910  26.50   98.87   567.7  0.2098  0.8663   \n",
      "4    0.1809  0.05883  ...  22.540  16.67  152.20  1575.0  0.1374  0.2050   \n",
      "..      ...      ...  ...     ...    ...     ...     ...     ...     ...   \n",
      "113  0.1922  0.07782  ...  11.160  22.75   72.62   374.4  0.1300  0.2049   \n",
      "114  0.1649  0.07633  ...   9.628  19.62   64.48   284.4  0.1724  0.2364   \n",
      "115  0.1688  0.06194  ...  13.670  26.15   87.54   583.0  0.1500  0.2399   \n",
      "116  0.1305  0.07163  ...   9.414  17.07   63.34   270.0  0.1179  0.1879   \n",
      "120  0.1667  0.06113  ...  12.820  15.97   83.74   510.5  0.1548  0.2390   \n",
      "\n",
      "         28       29      30       31  \n",
      "0    0.7119  0.26540  0.4601  0.11890  \n",
      "1    0.2416  0.18600  0.2750  0.08902  \n",
      "2    0.4504  0.24300  0.3613  0.08758  \n",
      "3    0.6869  0.25750  0.6638  0.17300  \n",
      "4    0.4000  0.16250  0.2364  0.07678  \n",
      "..      ...      ...     ...      ...  \n",
      "113  0.1295  0.06136  0.2383  0.09026  \n",
      "114  0.2456  0.10500  0.2926  0.10170  \n",
      "115  0.1503  0.07247  0.2438  0.08541  \n",
      "116  0.1544  0.03846  0.1652  0.07722  \n",
      "120  0.2102  0.08958  0.3016  0.08523  \n",
      "\n",
      "[80 rows x 30 columns] 0      0\n",
      "1      0\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "      ..\n",
      "113    1\n",
      "114    1\n",
      "115    1\n",
      "116    1\n",
      "120    1\n",
      "Name: 1, Length: 80, dtype: int64\n",
      "        2      3       4       5        6        7        8        9       10  \\\n",
      "33   19.27  26.47  127.90  1162.0  0.09401  0.17190  0.16570  0.07593  0.1853   \n",
      "34   16.13  17.88  107.00   807.2  0.10400  0.15590  0.13540  0.07752  0.1998   \n",
      "35   16.74  21.59  110.10   869.5  0.09610  0.13360  0.13480  0.06018  0.1896   \n",
      "36   14.25  21.72   93.63   633.0  0.09823  0.10980  0.13190  0.05598  0.1885   \n",
      "38   14.99  25.20   95.54   698.8  0.09387  0.05131  0.02398  0.02899  0.1565   \n",
      "..     ...    ...     ...     ...      ...      ...      ...      ...     ...   \n",
      "558  14.59  22.68   96.39   657.1  0.08473  0.13300  0.10290  0.03736  0.1454   \n",
      "559  11.51  23.93   74.52   403.5  0.09261  0.10210  0.11120  0.04105  0.1388   \n",
      "560  14.05  27.15   91.38   600.4  0.09929  0.11260  0.04462  0.04304  0.1537   \n",
      "561  11.20  29.37   70.67   386.0  0.07449  0.03558  0.00000  0.00000  0.1060   \n",
      "568   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000  0.00000  0.1587   \n",
      "\n",
      "          11  ...      22     23      24      25       26       27       28  \\\n",
      "33   0.06261  ...  24.150  30.90  161.40  1813.0  0.15090  0.65900  0.60910   \n",
      "34   0.06515  ...  20.210  27.26  132.70  1261.0  0.14460  0.58040  0.52740   \n",
      "35   0.05656  ...  20.010  29.02  133.50  1229.0  0.15630  0.38350  0.54090   \n",
      "36   0.06125  ...  15.890  30.36  116.20   799.6  0.14460  0.42380  0.51860   \n",
      "38   0.05504  ...  14.990  25.20   95.54   698.8  0.09387  0.05131  0.02398   \n",
      "..       ...  ...     ...    ...     ...     ...      ...      ...      ...   \n",
      "558  0.06147  ...  15.480  27.27  105.90   733.5  0.10260  0.31710  0.36620   \n",
      "559  0.06570  ...  12.480  37.16   82.28   474.2  0.12980  0.25170  0.36300   \n",
      "560  0.06171  ...  15.300  33.17  100.20   706.7  0.12410  0.22640  0.13260   \n",
      "561  0.05502  ...  11.920  38.30   75.19   439.6  0.09267  0.05494  0.00000   \n",
      "568  0.05884  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.00000   \n",
      "\n",
      "          29      30       31  \n",
      "33   0.17850  0.3672  0.11230  \n",
      "34   0.18640  0.4270  0.12330  \n",
      "35   0.18130  0.4863  0.08633  \n",
      "36   0.14470  0.3591  0.10140  \n",
      "38   0.02899  0.1565  0.05504  \n",
      "..       ...     ...      ...  \n",
      "558  0.11050  0.2258  0.08004  \n",
      "559  0.09653  0.2112  0.08732  \n",
      "560  0.10480  0.2250  0.08321  \n",
      "561  0.00000  0.1566  0.05905  \n",
      "568  0.00000  0.2871  0.07039  \n",
      "\n",
      "[489 rows x 30 columns] 33     0\n",
      "34     0\n",
      "35     0\n",
      "36     0\n",
      "38     0\n",
      "      ..\n",
      "558    1\n",
      "559    1\n",
      "560    1\n",
      "561    1\n",
      "568    1\n",
      "Name: 1, Length: 489, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mal = data[data[1] == 0]\n",
    "\n",
    "test_mal = mal[0:30]\n",
    "train_mal = mal[30:]\n",
    "\n",
    "\n",
    "ben = data[data[1] == 1 ]\n",
    "\n",
    "test_ben = ben[0:50]\n",
    "train_ben = ben[50:]\n",
    "\n",
    "\n",
    "test = pd.concat([test_mal, test_ben])\n",
    "x_test = test.iloc[:, 2:]\n",
    "y_test = test.iloc[:, 1]\n",
    "print(x_test, y_test)\n",
    "train = pd.concat([train_mal, train_ben])\n",
    "x_train = train.loc[:, 2:]\n",
    "\n",
    "y_train = train.loc[:, 1]\n",
    "y = [1]\n",
    "print(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c(i) Binary Classification Using Logistic Regression3\n",
    "\n",
    "Depict scatter plots of the features in your training set in a scatter matrix.(See p. 129 of the textbook). (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "# AB & NO\n",
    "sb.pairplot(data, hue = 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c(ii)\n",
    "\n",
    "Use logistic regression4 to solve the binary classification problem. Report the confusion matrix, ROC, precision, recall, F1 score, and AUC for both the train and test data sets. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC \n",
      " 0.9875\n",
      "Train AUC \n",
      " 0.9529652351738241\n",
      "Test Roc_Curve\n",
      " [0.         0.03333333 1.        ] [0. 1. 1.]\n",
      "Train Roc_Curve\n",
      " [0.         0.97068404 1.        ] [0.         0.97068404 1.        ]\n",
      "Test CF Matrix\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        30\n",
      "           1       0.98      1.00      0.99        50\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.99      0.98      0.99        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "Train CF Matrix\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       182\n",
      "           1       0.96      0.97      0.96       307\n",
      "\n",
      "    accuracy                           0.95       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.95      0.95      0.95       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, confusion_matrix, classification_report  \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.discrete.discrete_model as Logit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "log = LogisticRegression(max_iter = 5000)\n",
    "log.fit(x_train, y_train)\n",
    "pred_test = log.predict(x_test)\n",
    "pred_train = log.predict(x_train)\n",
    "print('Test AUC \\n', metrics.accuracy_score(y_test, pred_test))\n",
    "print('Train AUC \\n', metrics.accuracy_score(y_train, pred_train))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test.reshape(-1,1), pos_label = 1)\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(y_train, pred_train.reshape(-1,1) , pos_label = 1)\n",
    "print('Test Roc_Curve\\n', fpr, tpr)\n",
    "print('Train Roc_Curve\\n', tpr1, tpr1)\n",
    "cf_matrix=confusion_matrix(y_test,pred_test)\n",
    "cf_matrix1=confusion_matrix(y_train,pred_train)\n",
    "\n",
    "print('Test CF Matrix\\n',classification_report(y_test,pred_test) )\n",
    "print('Train CF Matrix\\n', classification_report(y_train,pred_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1] [[1.927e+01 2.647e+01 1.279e+02 ... 1.785e-01 3.672e-01 1.123e-01]\n",
      " [1.613e+01 1.788e+01 1.070e+02 ... 1.864e-01 4.270e-01 1.233e-01]\n",
      " [1.674e+01 2.159e+01 1.101e+02 ... 1.813e-01 4.863e-01 8.633e-02]\n",
      " ...\n",
      " [1.405e+01 2.715e+01 9.138e+01 ... 1.048e-01 2.250e-01 8.321e-02]\n",
      " [1.120e+01 2.937e+01 7.067e+01 ... 0.000e+00 1.566e-01 5.905e-02]\n",
      " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train.values, x_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c(iii)\n",
    "\n",
    "Calculate the p-values for your logistic regression parameters and prune those variables that are not statistically significant. Refit a logistic regression model using your pruned set of features.5 Report the confusion matrix, ROC, precision,recall, F1 score, and AUC for both the train and test data sets. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tara/opt/anaconda3/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:1799: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "/Users/tara/opt/anaconda3/lib/python3.8/site-packages/statsmodels/discrete/discrete_model.py:1852: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(self.cdf(q*np.dot(X,params))))\n",
      "/Users/tara/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/optimize.py:2116: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/Users/tara/opt/anaconda3/lib/python3.8/site-packages/scipy/optimize/optimize.py:2117: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  p = (x - v) * tmp2 - (x - w) * tmp1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  489\n",
      "Model:                          Logit   Df Residuals:                      459\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Fri, 26 Mar 2021   Pseudo R-squ.:                  0.9189\n",
      "Time:                        19:36:32   Log-Likelihood:                -26.163\n",
      "converged:                      False   LL-Null:                       -322.79\n",
      "Covariance Type:            nonrobust   LLR p-value:                1.609e-106\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.3480     12.233      0.028      0.977     -23.629      24.325\n",
      "x2             0.0226      0.359      0.063      0.950      -0.681       0.727\n",
      "x3             0.1643      1.750      0.094      0.925      -3.266       3.595\n",
      "x4             0.0019      0.058      0.033      0.974      -0.112       0.116\n",
      "x5            28.0842    117.049      0.240      0.810    -201.327     257.495\n",
      "x6            52.7772     74.037      0.713      0.476     -92.333     197.888\n",
      "x7           -36.0698     77.732     -0.464      0.643    -188.421     116.281\n",
      "x8          -140.0698    108.316     -1.293      0.196    -352.366      72.226\n",
      "x9            -1.0438     36.794     -0.028      0.977     -73.158      71.071\n",
      "x10          179.7534    272.671      0.659      0.510    -354.672     714.179\n",
      "x11           10.1633     42.830      0.237      0.812     -73.782      94.109\n",
      "x12            1.9792      2.225      0.890      0.374      -2.381       6.340\n",
      "x13            0.7556      3.308      0.228      0.819      -5.728       7.239\n",
      "x14           -0.3624      0.387     -0.937      0.349      -1.120       0.396\n",
      "x15         -365.4517    389.593     -0.938      0.348   -1129.041     398.137\n",
      "x16          203.3039    182.794      1.112      0.266    -154.966     561.574\n",
      "x17           46.2923     97.112      0.477      0.634    -144.045     236.629\n",
      "x18           41.6907    552.131      0.076      0.940   -1040.466    1123.848\n",
      "x19          -36.3738    186.806     -0.195      0.846    -402.507     329.759\n",
      "x20         -570.8733    976.493     -0.585      0.559   -2484.764    1343.018\n",
      "x21           -0.5521      5.811     -0.095      0.924     -11.942      10.838\n",
      "x22           -0.3533      0.315     -1.122      0.262      -0.970       0.264\n",
      "x23            0.0948      0.465      0.204      0.839      -0.817       1.007\n",
      "x24           -0.0122      0.048     -0.253      0.800      -0.106       0.082\n",
      "x25           19.0536     65.231      0.292      0.770    -108.797     146.904\n",
      "x26          -19.3205     31.407     -0.615      0.538     -80.877      42.236\n",
      "x27          -10.8516     13.144     -0.826      0.409     -36.613      14.910\n",
      "x28          -16.2331     59.829     -0.271      0.786    -133.496     101.029\n",
      "x29           -6.6269     20.798     -0.319      0.750     -47.391      34.137\n",
      "x30          -25.2193    150.097     -0.168      0.867    -319.403     268.964\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.53 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tara/opt/anaconda3/lib/python3.8/site-packages/statsmodels/base/model.py:566: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC \n",
      " 0.9875\n",
      "Train AUC \n",
      " 0.9529652351738241\n",
      "Test Roc_Curve\n",
      " [0.         0.03333333 1.        ] [0. 1. 1.]\n",
      "Train Roc_Curve\n",
      " [0.         0.97068404 1.        ] [0.         0.97068404 1.        ]\n",
      "Test CF Matrix\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        30\n",
      "           1       0.98      1.00      0.99        50\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.99      0.98      0.99        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "Train CF Matrix\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       182\n",
      "           1       0.96      0.97      0.96       307\n",
      "\n",
      "    accuracy                           0.95       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.95      0.95      0.95       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, confusion_matrix, classification_report  \n",
    "import statsmodels.api as sm\n",
    "import statsmodels.discrete.discrete_model as Logit\n",
    "\n",
    "\n",
    "model = sm.Logit(y_train.values, x_train.values)\n",
    "\n",
    "p_value1 = model.fit(maxiter = 20, method = 'powell')\n",
    "print(p_value1.summary())\n",
    "\n",
    "# I went to Dhruvin Shah's OH and we worked for at least 30mins\n",
    "# trying to figure out why my pvalues would not give me any significant values\n",
    "# my model.fit() won't work without a method other than newton being set\n",
    "# I even asked my father who is the chief data officer at CHLA and we couldn't find the issue\n",
    "# therefore I refit my logisticRegression but it would be exactly the same as above \n",
    "\n",
    "\n",
    "log = LogisticRegression(max_iter = 5000)\n",
    "log.fit(x_train, y_train)\n",
    "\n",
    "pred_test = log.predict(x_test)\n",
    "pred_train = log.predict(x_train)\n",
    "print('Test AUC \\n', metrics.accuracy_score(y_test, pred_test))\n",
    "print('Train AUC \\n', metrics.accuracy_score(y_train, pred_train))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test.reshape(-1,1), pos_label = 1)\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(y_train, pred_train.reshape(-1,1) , pos_label = 1)\n",
    "print('Test Roc_Curve\\n', fpr, tpr)\n",
    "print('Train Roc_Curve\\n', tpr1, tpr1)\n",
    "cf_matrix=confusion_matrix(y_test,pred_test)\n",
    "cf_matrix1=confusion_matrix(y_train,pred_train)\n",
    "\n",
    "print('Test CF Matrix\\n',classification_report(y_test,pred_test) )\n",
    "print('Train CF Matrix\\n', classification_report(y_train,pred_train) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.c(iv)\n",
    "\n",
    "Do your classes seem to be well-separated to cause instability in calculating logistic regression parameters? (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My classes seem to be well-seperated because my maximum likelihood cannot converge, that leads me to believe my classes are well-seperated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.a\n",
    "\n",
    "Download the WPBC data from: https://archive.ics.uci.edu/ml/datasets/\n",
    "Breast+Cancer+Wisconsin+(Diagnostic). (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119513</td>\n",
       "      <td>N</td>\n",
       "      <td>31</td>\n",
       "      <td>18.02</td>\n",
       "      <td>27.60</td>\n",
       "      <td>117.50</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.09489</td>\n",
       "      <td>0.10360</td>\n",
       "      <td>0.10860</td>\n",
       "      <td>...</td>\n",
       "      <td>139.70</td>\n",
       "      <td>1436.0</td>\n",
       "      <td>0.11950</td>\n",
       "      <td>0.1926</td>\n",
       "      <td>0.3140</td>\n",
       "      <td>0.11700</td>\n",
       "      <td>0.2677</td>\n",
       "      <td>0.08113</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8423</td>\n",
       "      <td>N</td>\n",
       "      <td>61</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>...</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.26540</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>842517</td>\n",
       "      <td>N</td>\n",
       "      <td>116</td>\n",
       "      <td>21.37</td>\n",
       "      <td>17.44</td>\n",
       "      <td>137.50</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>0.08836</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.12550</td>\n",
       "      <td>...</td>\n",
       "      <td>159.10</td>\n",
       "      <td>1949.0</td>\n",
       "      <td>0.11880</td>\n",
       "      <td>0.3449</td>\n",
       "      <td>0.3414</td>\n",
       "      <td>0.20320</td>\n",
       "      <td>0.4334</td>\n",
       "      <td>0.09067</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>843483</td>\n",
       "      <td>N</td>\n",
       "      <td>123</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>...</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.25750</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>843584</td>\n",
       "      <td>R</td>\n",
       "      <td>27</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>...</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.16250</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>942640</td>\n",
       "      <td>N</td>\n",
       "      <td>10</td>\n",
       "      <td>22.52</td>\n",
       "      <td>21.92</td>\n",
       "      <td>146.90</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>0.07592</td>\n",
       "      <td>0.09162</td>\n",
       "      <td>0.06862</td>\n",
       "      <td>...</td>\n",
       "      <td>162.10</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>0.08191</td>\n",
       "      <td>0.1319</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.09378</td>\n",
       "      <td>0.2061</td>\n",
       "      <td>0.05788</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>943471</td>\n",
       "      <td>N</td>\n",
       "      <td>8</td>\n",
       "      <td>15.44</td>\n",
       "      <td>31.18</td>\n",
       "      <td>101.00</td>\n",
       "      <td>740.4</td>\n",
       "      <td>0.09399</td>\n",
       "      <td>0.10620</td>\n",
       "      <td>0.13750</td>\n",
       "      <td>...</td>\n",
       "      <td>112.60</td>\n",
       "      <td>929.0</td>\n",
       "      <td>0.12720</td>\n",
       "      <td>0.2362</td>\n",
       "      <td>0.2975</td>\n",
       "      <td>0.12860</td>\n",
       "      <td>0.2914</td>\n",
       "      <td>0.08024</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>94547</td>\n",
       "      <td>N</td>\n",
       "      <td>12</td>\n",
       "      <td>17.17</td>\n",
       "      <td>29.19</td>\n",
       "      <td>110.00</td>\n",
       "      <td>915.3</td>\n",
       "      <td>0.08952</td>\n",
       "      <td>0.06655</td>\n",
       "      <td>0.06583</td>\n",
       "      <td>...</td>\n",
       "      <td>132.50</td>\n",
       "      <td>1295.0</td>\n",
       "      <td>0.12610</td>\n",
       "      <td>0.1572</td>\n",
       "      <td>0.2141</td>\n",
       "      <td>0.09520</td>\n",
       "      <td>0.3362</td>\n",
       "      <td>0.06033</td>\n",
       "      <td>3.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>947204</td>\n",
       "      <td>R</td>\n",
       "      <td>3</td>\n",
       "      <td>21.42</td>\n",
       "      <td>22.84</td>\n",
       "      <td>145.00</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>0.10700</td>\n",
       "      <td>0.19390</td>\n",
       "      <td>0.23800</td>\n",
       "      <td>...</td>\n",
       "      <td>198.30</td>\n",
       "      <td>2375.0</td>\n",
       "      <td>0.14980</td>\n",
       "      <td>0.4379</td>\n",
       "      <td>0.5411</td>\n",
       "      <td>0.22150</td>\n",
       "      <td>0.2832</td>\n",
       "      <td>0.08981</td>\n",
       "      <td>3.0</td>\n",
       "      <td>?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>947489</td>\n",
       "      <td>N</td>\n",
       "      <td>6</td>\n",
       "      <td>16.70</td>\n",
       "      <td>28.13</td>\n",
       "      <td>110.30</td>\n",
       "      <td>885.4</td>\n",
       "      <td>0.08896</td>\n",
       "      <td>0.11310</td>\n",
       "      <td>0.10120</td>\n",
       "      <td>...</td>\n",
       "      <td>128.80</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>0.13300</td>\n",
       "      <td>0.2808</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>0.13170</td>\n",
       "      <td>0.3035</td>\n",
       "      <td>0.08036</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1    2      3      4       5       6        7        8        9   \\\n",
       "0    119513  N   31  18.02  27.60  117.50  1013.0  0.09489  0.10360  0.10860   \n",
       "1      8423  N   61  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "2    842517  N  116  21.37  17.44  137.50  1373.0  0.08836  0.11890  0.12550   \n",
       "3    843483  N  123  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4    843584  R   27  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "..      ... ..  ...    ...    ...     ...     ...      ...      ...      ...   \n",
       "193  942640  N   10  22.52  21.92  146.90  1597.0  0.07592  0.09162  0.06862   \n",
       "194  943471  N    8  15.44  31.18  101.00   740.4  0.09399  0.10620  0.13750   \n",
       "195   94547  N   12  17.17  29.19  110.00   915.3  0.08952  0.06655  0.06583   \n",
       "196  947204  R    3  21.42  22.84  145.00  1440.0  0.10700  0.19390  0.23800   \n",
       "197  947489  N    6  16.70  28.13  110.30   885.4  0.08896  0.11310  0.10120   \n",
       "\n",
       "     ...      25      26       27      28      29       30      31       32  \\\n",
       "0    ...  139.70  1436.0  0.11950  0.1926  0.3140  0.11700  0.2677  0.08113   \n",
       "1    ...  184.60  2019.0  0.16220  0.6656  0.7119  0.26540  0.4601  0.11890   \n",
       "2    ...  159.10  1949.0  0.11880  0.3449  0.3414  0.20320  0.4334  0.09067   \n",
       "3    ...   98.87   567.7  0.20980  0.8663  0.6869  0.25750  0.6638  0.17300   \n",
       "4    ...  152.20  1575.0  0.13740  0.2050  0.4000  0.16250  0.2364  0.07678   \n",
       "..   ...     ...     ...      ...     ...     ...      ...     ...      ...   \n",
       "193  ...  162.10  1902.0  0.08191  0.1319  0.1056  0.09378  0.2061  0.05788   \n",
       "194  ...  112.60   929.0  0.12720  0.2362  0.2975  0.12860  0.2914  0.08024   \n",
       "195  ...  132.50  1295.0  0.12610  0.1572  0.2141  0.09520  0.3362  0.06033   \n",
       "196  ...  198.30  2375.0  0.14980  0.4379  0.5411  0.22150  0.2832  0.08981   \n",
       "197  ...  128.80  1213.0  0.13300  0.2808  0.3455  0.13170  0.3035  0.08036   \n",
       "\n",
       "      33  34  \n",
       "0    5.0   5  \n",
       "1    3.0   2  \n",
       "2    2.5   0  \n",
       "3    2.0   0  \n",
       "4    3.5   0  \n",
       "..   ...  ..  \n",
       "193  6.0   2  \n",
       "194  1.5   0  \n",
       "195  3.7   0  \n",
       "196  3.0   ?  \n",
       "197  3.5   0  \n",
       "\n",
       "[198 rows x 35 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = pd.read_csv('Lab 3 Data/wpbc.data', header = None);\n",
    "data2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.b\n",
    "\n",
    "Select the first 130 non-recurrent cases and the first 37 recurrent cases as your training set. Add record #197 in the data set to your training set as well. (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0  1   2      3      4       5       6        7        8        9   \\\n",
      "37    857010  R   8  18.65  17.60  123.70  1076.0  0.10990  0.16860  0.19740   \n",
      "39    857438  R  48  15.10  22.02   97.26   712.8  0.09056  0.07081  0.05253   \n",
      "40    857637  R  11  19.21  18.57  125.50  1152.0  0.10530  0.12670  0.13230   \n",
      "42    858986  R  34  14.25  22.15   96.42   645.7  0.10490  0.20080  0.21350   \n",
      "43    859223  R  19  12.68  23.84   82.69   499.0  0.11220  0.12620  0.11280   \n",
      "46    859983  R  26  13.80  15.79   90.43   584.1  0.10070  0.12800  0.07789   \n",
      "48   8611792  R  40  19.10  26.29  129.10  1132.0  0.12150  0.17910  0.19370   \n",
      "50     86208  R  10  20.77  22.83  137.40  1336.0  0.10330  0.15150  0.16370   \n",
      "56    865128  R  35  17.95  20.01  114.20   982.0  0.08402  0.06722  0.07293   \n",
      "59    866203  R  73  19.00  18.91  123.40  1138.0  0.08217  0.08028  0.09271   \n",
      "62    867739  R   7  20.29  21.49  132.80  1291.0  0.09258  0.12050  0.15230   \n",
      "67     87112  R  44  17.68  20.74  117.40   963.7  0.11150  0.16650  0.18550   \n",
      "69    871201  R   8  19.59  18.15  130.70  1214.0  0.11200  0.16660  0.25080   \n",
      "75    873592  R  17  27.22  21.87  182.10  2250.0  0.10940  0.19140  0.28710   \n",
      "81    875263  R  74  12.34  26.86   81.15   477.4  0.10340  0.13530  0.10850   \n",
      "88    879523  R  17  15.12  16.68   98.78   716.6  0.08876  0.09588  0.07550   \n",
      "105   886490  R  19  19.55  28.77  133.60  1207.0  0.09260  0.20630  0.17840   \n",
      "109   887549  R  39  20.31  27.06  132.90  1288.0  0.10000  0.10880  0.15190   \n",
      "110   888570  R  12  17.29  22.13  114.40   947.8  0.08999  0.12730  0.09697   \n",
      "112   889719  R  37  17.19  22.07  111.60   928.3  0.09726  0.08995  0.09061   \n",
      "114  8910988  R   5  21.75  20.99  147.30  1491.0  0.09401  0.19610  0.21950   \n",
      "120   892438  R  58  19.53  18.90  129.50  1217.0  0.11500  0.16420  0.21970   \n",
      "125    89539  R  78  16.27  20.71  106.90   813.7  0.11690  0.13190  0.14780   \n",
      "128    89812  R  24  23.51  24.27  155.10  1747.0  0.10690  0.12830  0.23080   \n",
      "129   898431  R  33  19.68  21.68  129.90  1194.0  0.09797  0.13390  0.18630   \n",
      "131   899987  R   4  25.73  17.46  174.20  2010.0  0.11490  0.23630  0.33680   \n",
      "132  9010018  R   5  15.08  25.74   98.00   716.6  0.10240  0.09769  0.12350   \n",
      "135  9012000  R   2  22.01  21.90  147.20  1482.0  0.10630  0.19540  0.24480   \n",
      "143   906023  R  49  15.50  21.08  102.90   803.1  0.11200  0.15710  0.15220   \n",
      "145   908194  R   1  20.18  19.54  133.80  1250.0  0.11330  0.14890  0.21330   \n",
      "147   908489  R   9  13.98  19.62   91.12   599.5  0.10600  0.11330  0.11260   \n",
      "150  9110732  R  16  17.75  28.03  117.30   981.6  0.09997  0.13140  0.16980   \n",
      "152  9111805  R   9  19.59  25.00  127.70  1191.0  0.10320  0.09871  0.16550   \n",
      "155   913505  R  14  19.44  18.82  128.10  1167.0  0.10890  0.14480  0.22560   \n",
      "157   914062  R  12  18.01  20.56  118.40  1007.0  0.10010  0.12890  0.11700   \n",
      "159    91485  R  11  20.59  21.24  137.80  1320.0  0.10850  0.16440  0.21880   \n",
      "161   915143  R   7  24.24  18.74  159.60  1945.0  0.08938  0.11360  0.17270   \n",
      "176   929684  R  14  17.53  25.28  114.00   966.6  0.09278  0.09175  0.11050   \n",
      "177   931652  R   1  18.11  30.99  115.80   984.4  0.08625  0.09240  0.06214   \n",
      "\n",
      "     ...     25      26      27      28      29      30      31       32  \\\n",
      "37   ...  150.6  1567.0  0.1679  0.5090  0.7345  0.2378  0.3799  0.09185   \n",
      "39   ...  117.7  1030.0  0.1389  0.2057  0.2712  0.1530  0.2675  0.07873   \n",
      "40   ...  170.1  2145.0  0.1624  0.3511  0.3879  0.2091  0.3537  0.08294   \n",
      "42   ...  119.1   959.5  0.1640  0.6247  0.6922  0.1785  0.2844  0.11320   \n",
      "43   ...  111.8   888.3  0.1851  0.4061  0.4024  0.1716  0.3383  0.10310   \n",
      "46   ...  110.3   812.4  0.1411  0.3542  0.2779  0.1383  0.2589  0.10300   \n",
      "48   ...  141.3  1298.0  0.1392  0.2817  0.2432  0.1841  0.2311  0.09203   \n",
      "50   ...  159.9  1816.0  0.1385  0.4107  0.3757  0.1881  0.3371  0.07651   \n",
      "56   ...  129.2  1261.0  0.1072  0.1202  0.2249  0.1185  0.4882  0.06111   \n",
      "59   ...  148.2  1538.0  0.1021  0.2264  0.3207  0.1218  0.2841  0.06541   \n",
      "62   ...  171.6  2196.0  0.1225  0.3449  0.4509  0.1943  0.4416  0.07863   \n",
      "67   ...  132.9  1302.0  0.1418  0.3498  0.3583  0.1515  0.2463  0.07738   \n",
      "69   ...  174.9  2232.0  0.1438  0.3846  0.6810  0.2247  0.3643  0.09223   \n",
      "75   ...  220.8  3216.0  0.1472  0.4034  0.5340  0.2688  0.2856  0.08082   \n",
      "81   ...  101.7   768.9  0.1785  0.4706  0.4425  0.1459  0.3215  0.12050   \n",
      "88   ...  117.7   989.5  0.1491  0.3331  0.3327  0.1252  0.3415  0.09740   \n",
      "105  ...  178.6  1926.0  0.1281  0.5329  0.4251  0.1941  0.2818  0.10050   \n",
      "109  ...  162.3  1844.0  0.1522  0.2945  0.3788  0.1697  0.3151  0.07999   \n",
      "110  ...  137.9  1295.0  0.1134  0.2867  0.2298  0.1528  0.3067  0.07484   \n",
      "112  ...  140.5  1436.0  0.1558  0.2567  0.3889  0.1984  0.3216  0.07570   \n",
      "114  ...  195.9  2384.0  0.1272  0.4725  0.5807  0.1841  0.2833  0.08858   \n",
      "120  ...  171.1  2053.0  0.1495  0.4116  0.6121  0.1980  0.2968  0.09929   \n",
      "125  ...  129.8  1121.0  0.1590  0.2947  0.3597  0.1583  0.3103  0.08200   \n",
      "128  ...  202.4  2906.0  0.1515  0.2678  0.4819  0.2089  0.2593  0.07738   \n",
      "129  ...  157.6  1540.0  0.1218  0.3458  0.4734  0.2255  0.4045  0.07918   \n",
      "131  ...  229.3  3234.0  0.1530  0.5937  0.6451  0.2756  0.3690  0.08815   \n",
      "132  ...  121.2  1050.0  0.1660  0.2356  0.4029  0.1526  0.2654  0.09438   \n",
      "135  ...  195.0  2227.0  0.1294  0.3885  0.4756  0.2432  0.2741  0.08574   \n",
      "143  ...  157.1  1748.0  0.1517  0.4002  0.4211  0.2134  0.3003  0.10480   \n",
      "145  ...  146.0  1479.0  0.1665  0.2942  0.5308  0.2173  0.3032  0.08075   \n",
      "147  ...  113.9   869.3  0.1613  0.3568  0.4069  0.1827  0.3179  0.10550   \n",
      "150  ...  145.4  1437.0  0.1401  0.3762  0.6399  0.1970  0.2972  0.09075   \n",
      "152  ...  139.8  1421.0  0.1528  0.1845  0.3977  0.1466  0.2293  0.06091   \n",
      "155  ...  153.9  1740.0  0.1514  0.3725  0.5936  0.2060  0.3266  0.09009   \n",
      "157  ...  143.4  1426.0  0.1309  0.2327  0.2544  0.1489  0.3251  0.07625   \n",
      "159  ...  163.2  1760.0  0.1464  0.3597  0.5179  0.2113  0.2480  0.08999   \n",
      "161  ...  232.2  3903.0  0.1154  0.1772  0.2917  0.1795  0.2336  0.06259   \n",
      "176  ...  142.6  1483.0  0.1287  0.2472  0.2753  0.1372  0.2404  0.07156   \n",
      "177  ...  128.0  1214.0  0.1194  0.2088  0.2385  0.1333  0.2652  0.07006   \n",
      "\n",
      "       33  34  \n",
      "37    1.8   0  \n",
      "39    2.5   0  \n",
      "40    3.2  13  \n",
      "42    1.5   0  \n",
      "43    1.2   1  \n",
      "46    3.0   2  \n",
      "48    3.0   1  \n",
      "50    4.0   2  \n",
      "56    3.0   1  \n",
      "59    3.5  15  \n",
      "62    2.5   9  \n",
      "67    9.0   7  \n",
      "69    3.5   0  \n",
      "75    4.0   4  \n",
      "81    1.2   4  \n",
      "88    1.5   0  \n",
      "105   6.0  15  \n",
      "109   1.8   1  \n",
      "110   4.0   1  \n",
      "112   8.5   6  \n",
      "114   2.5  11  \n",
      "120   2.0   0  \n",
      "125   2.5   2  \n",
      "128   3.5   4  \n",
      "129   1.5  13  \n",
      "131   4.5   0  \n",
      "132   2.6   0  \n",
      "135   3.0   1  \n",
      "143   0.4   0  \n",
      "145   4.5  27  \n",
      "147   3.5   4  \n",
      "150   1.5   7  \n",
      "152   2.0   7  \n",
      "155   7.0   9  \n",
      "157   3.5   2  \n",
      "159   5.5  20  \n",
      "161   3.0   4  \n",
      "176  10.0   9  \n",
      "177   2.7   4  \n",
      "\n",
      "[39 rows x 35 columns]\n",
      "hello           0  1   2      3      4       5       6        7        8        9   \\\n",
      "174   927997  N  27  18.11  26.17  118.70   970.2  0.09867  0.17160  0.21770   \n",
      "175   928594  N  29  21.93  30.64  146.70  1487.0  0.08679  0.17230  0.20530   \n",
      "178   931678  N  24  24.29  25.48  161.80  1715.0  0.09374  0.22840  0.27020   \n",
      "179   935058  N  13  15.60  26.79   99.85   760.0  0.07885  0.05240  0.03778   \n",
      "180   935878  N  13  15.78  17.10  102.20   769.0  0.09668  0.09030  0.07268   \n",
      "181   937100  N  12  19.28  20.88  124.90  1127.0  0.09033  0.11970  0.06435   \n",
      "182   937653  N  15  15.66  24.51  102.00   771.1  0.08886  0.08731  0.09483   \n",
      "183   937654  N  11  22.44  27.42  150.60  1504.0  0.12110  0.20820  0.35790   \n",
      "184   937664  N  17  17.98  23.96  120.00   995.0  0.11570  0.17390  0.19540   \n",
      "185   937897  N  17  13.63  24.70   89.65   569.2  0.10550  0.13120  0.11610   \n",
      "186   938225  N   9  23.01  33.87  156.80  1705.0  0.11570  0.19480  0.29790   \n",
      "187   938412  N  10  22.41  29.95  145.50  1528.0  0.11190  0.16990  0.30760   \n",
      "188   938413  N  15  12.53  30.98   80.41   490.9  0.09252  0.06271  0.06151   \n",
      "189   939095  N   6  19.80  20.46  130.20  1235.0  0.09652  0.10770  0.15990   \n",
      "190   939426  N   8  19.96  27.41  130.80  1238.0  0.09075  0.11670  0.13550   \n",
      "191  9411286  N   3  19.22  27.18  128.80  1134.0  0.10900  0.17770  0.21380   \n",
      "192  9411300  N   3  14.72  25.26   99.28   657.5  0.11740  0.21120  0.17290   \n",
      "193   942640  N  10  22.52  21.92  146.90  1597.0  0.07592  0.09162  0.06862   \n",
      "194   943471  N   8  15.44  31.18  101.00   740.4  0.09399  0.10620  0.13750   \n",
      "195    94547  N  12  17.17  29.19  110.00   915.3  0.08952  0.06655  0.06583   \n",
      "197   947489  N   6  16.70  28.13  110.30   885.4  0.08896  0.11310  0.10120   \n",
      "37    857010  R   8  18.65  17.60  123.70  1076.0  0.10990  0.16860  0.19740   \n",
      "39    857438  R  48  15.10  22.02   97.26   712.8  0.09056  0.07081  0.05253   \n",
      "40    857637  R  11  19.21  18.57  125.50  1152.0  0.10530  0.12670  0.13230   \n",
      "42    858986  R  34  14.25  22.15   96.42   645.7  0.10490  0.20080  0.21350   \n",
      "43    859223  R  19  12.68  23.84   82.69   499.0  0.11220  0.12620  0.11280   \n",
      "46    859983  R  26  13.80  15.79   90.43   584.1  0.10070  0.12800  0.07789   \n",
      "48   8611792  R  40  19.10  26.29  129.10  1132.0  0.12150  0.17910  0.19370   \n",
      "50     86208  R  10  20.77  22.83  137.40  1336.0  0.10330  0.15150  0.16370   \n",
      "56    865128  R  35  17.95  20.01  114.20   982.0  0.08402  0.06722  0.07293   \n",
      "59    866203  R  73  19.00  18.91  123.40  1138.0  0.08217  0.08028  0.09271   \n",
      "62    867739  R   7  20.29  21.49  132.80  1291.0  0.09258  0.12050  0.15230   \n",
      "67     87112  R  44  17.68  20.74  117.40   963.7  0.11150  0.16650  0.18550   \n",
      "69    871201  R   8  19.59  18.15  130.70  1214.0  0.11200  0.16660  0.25080   \n",
      "75    873592  R  17  27.22  21.87  182.10  2250.0  0.10940  0.19140  0.28710   \n",
      "81    875263  R  74  12.34  26.86   81.15   477.4  0.10340  0.13530  0.10850   \n",
      "88    879523  R  17  15.12  16.68   98.78   716.6  0.08876  0.09588  0.07550   \n",
      "105   886490  R  19  19.55  28.77  133.60  1207.0  0.09260  0.20630  0.17840   \n",
      "109   887549  R  39  20.31  27.06  132.90  1288.0  0.10000  0.10880  0.15190   \n",
      "110   888570  R  12  17.29  22.13  114.40   947.8  0.08999  0.12730  0.09697   \n",
      "112   889719  R  37  17.19  22.07  111.60   928.3  0.09726  0.08995  0.09061   \n",
      "114  8910988  R   5  21.75  20.99  147.30  1491.0  0.09401  0.19610  0.21950   \n",
      "120   892438  R  58  19.53  18.90  129.50  1217.0  0.11500  0.16420  0.21970   \n",
      "125    89539  R  78  16.27  20.71  106.90   813.7  0.11690  0.13190  0.14780   \n",
      "128    89812  R  24  23.51  24.27  155.10  1747.0  0.10690  0.12830  0.23080   \n",
      "129   898431  R  33  19.68  21.68  129.90  1194.0  0.09797  0.13390  0.18630   \n",
      "131   899987  R   4  25.73  17.46  174.20  2010.0  0.11490  0.23630  0.33680   \n",
      "132  9010018  R   5  15.08  25.74   98.00   716.6  0.10240  0.09769  0.12350   \n",
      "135  9012000  R   2  22.01  21.90  147.20  1482.0  0.10630  0.19540  0.24480   \n",
      "143   906023  R  49  15.50  21.08  102.90   803.1  0.11200  0.15710  0.15220   \n",
      "145   908194  R   1  20.18  19.54  133.80  1250.0  0.11330  0.14890  0.21330   \n",
      "147   908489  R   9  13.98  19.62   91.12   599.5  0.10600  0.11330  0.11260   \n",
      "150  9110732  R  16  17.75  28.03  117.30   981.6  0.09997  0.13140  0.16980   \n",
      "152  9111805  R   9  19.59  25.00  127.70  1191.0  0.10320  0.09871  0.16550   \n",
      "155   913505  R  14  19.44  18.82  128.10  1167.0  0.10890  0.14480  0.22560   \n",
      "157   914062  R  12  18.01  20.56  118.40  1007.0  0.10010  0.12890  0.11700   \n",
      "159    91485  R  11  20.59  21.24  137.80  1320.0  0.10850  0.16440  0.21880   \n",
      "161   915143  R   7  24.24  18.74  159.60  1945.0  0.08938  0.11360  0.17270   \n",
      "176   929684  R  14  17.53  25.28  114.00   966.6  0.09278  0.09175  0.11050   \n",
      "177   931652  R   1  18.11  30.99  115.80   984.4  0.08625  0.09240  0.06214   \n",
      "\n",
      "     ...      25      26       27       28       29       30      31       32  \\\n",
      "174  ...  145.10  1251.0  0.14640  0.51270  0.87740  0.26030  0.3941  0.09318   \n",
      "175  ...  171.50  1951.0  0.11680  0.40720  0.44940  0.18860  0.2784  0.07353   \n",
      "178  ...  184.80  2213.0  0.12470  0.39350  0.61180  0.20630  0.3983  0.07978   \n",
      "179  ...  117.20  1027.0  0.10970  0.09745  0.12630  0.08222  0.2805  0.06378   \n",
      "180  ...  111.70   921.4  0.13860  0.26400  0.29010  0.12340  0.3261  0.07752   \n",
      "181  ...  130.00  1214.0  0.09273  0.12590  0.08201  0.11160  0.2147  0.05644   \n",
      "182  ...  124.00  1066.0  0.11820  0.24860  0.35080  0.12730  0.2964  0.06591   \n",
      "183  ...  161.20  1689.0  0.13300  0.25800  0.47410  0.21350  0.2809  0.06318   \n",
      "184  ...  135.20  1225.0  0.13400  0.26060  0.31240  0.17530  0.2746  0.07830   \n",
      "185  ...  107.70   712.2  0.15540  0.35150  0.34090  0.16890  0.2739  0.09945   \n",
      "186  ...  197.80  2481.0  0.14320  0.28950  0.49370  0.21100  0.2774  0.07542   \n",
      "187  ...  194.50  2690.0  0.12510  0.38320  0.58420  0.24220  0.2729  0.09391   \n",
      "188  ...   96.75   694.6  0.15600  0.13580  0.24360  0.12590  0.3292  0.09406   \n",
      "189  ...  155.10  1623.0  0.13730  0.23680  0.39310  0.18400  0.2382  0.06831   \n",
      "190  ...  150.30  1603.0  0.12930  0.25130  0.41060  0.16380  0.2245  0.07842   \n",
      "191  ...  161.00  1565.0  0.15010  0.51300  0.68410  0.20990  0.3484  0.09774   \n",
      "192  ...  111.60   814.8  0.14640  0.53520  0.56550  0.19740  0.3778  0.11320   \n",
      "193  ...  162.10  1902.0  0.08191  0.13190  0.10560  0.09378  0.2061  0.05788   \n",
      "194  ...  112.60   929.0  0.12720  0.23620  0.29750  0.12860  0.2914  0.08024   \n",
      "195  ...  132.50  1295.0  0.12610  0.15720  0.21410  0.09520  0.3362  0.06033   \n",
      "197  ...  128.80  1213.0  0.13300  0.28080  0.34550  0.13170  0.3035  0.08036   \n",
      "37   ...  150.60  1567.0  0.16790  0.50900  0.73450  0.23780  0.3799  0.09185   \n",
      "39   ...  117.70  1030.0  0.13890  0.20570  0.27120  0.15300  0.2675  0.07873   \n",
      "40   ...  170.10  2145.0  0.16240  0.35110  0.38790  0.20910  0.3537  0.08294   \n",
      "42   ...  119.10   959.5  0.16400  0.62470  0.69220  0.17850  0.2844  0.11320   \n",
      "43   ...  111.80   888.3  0.18510  0.40610  0.40240  0.17160  0.3383  0.10310   \n",
      "46   ...  110.30   812.4  0.14110  0.35420  0.27790  0.13830  0.2589  0.10300   \n",
      "48   ...  141.30  1298.0  0.13920  0.28170  0.24320  0.18410  0.2311  0.09203   \n",
      "50   ...  159.90  1816.0  0.13850  0.41070  0.37570  0.18810  0.3371  0.07651   \n",
      "56   ...  129.20  1261.0  0.10720  0.12020  0.22490  0.11850  0.4882  0.06111   \n",
      "59   ...  148.20  1538.0  0.10210  0.22640  0.32070  0.12180  0.2841  0.06541   \n",
      "62   ...  171.60  2196.0  0.12250  0.34490  0.45090  0.19430  0.4416  0.07863   \n",
      "67   ...  132.90  1302.0  0.14180  0.34980  0.35830  0.15150  0.2463  0.07738   \n",
      "69   ...  174.90  2232.0  0.14380  0.38460  0.68100  0.22470  0.3643  0.09223   \n",
      "75   ...  220.80  3216.0  0.14720  0.40340  0.53400  0.26880  0.2856  0.08082   \n",
      "81   ...  101.70   768.9  0.17850  0.47060  0.44250  0.14590  0.3215  0.12050   \n",
      "88   ...  117.70   989.5  0.14910  0.33310  0.33270  0.12520  0.3415  0.09740   \n",
      "105  ...  178.60  1926.0  0.12810  0.53290  0.42510  0.19410  0.2818  0.10050   \n",
      "109  ...  162.30  1844.0  0.15220  0.29450  0.37880  0.16970  0.3151  0.07999   \n",
      "110  ...  137.90  1295.0  0.11340  0.28670  0.22980  0.15280  0.3067  0.07484   \n",
      "112  ...  140.50  1436.0  0.15580  0.25670  0.38890  0.19840  0.3216  0.07570   \n",
      "114  ...  195.90  2384.0  0.12720  0.47250  0.58070  0.18410  0.2833  0.08858   \n",
      "120  ...  171.10  2053.0  0.14950  0.41160  0.61210  0.19800  0.2968  0.09929   \n",
      "125  ...  129.80  1121.0  0.15900  0.29470  0.35970  0.15830  0.3103  0.08200   \n",
      "128  ...  202.40  2906.0  0.15150  0.26780  0.48190  0.20890  0.2593  0.07738   \n",
      "129  ...  157.60  1540.0  0.12180  0.34580  0.47340  0.22550  0.4045  0.07918   \n",
      "131  ...  229.30  3234.0  0.15300  0.59370  0.64510  0.27560  0.3690  0.08815   \n",
      "132  ...  121.20  1050.0  0.16600  0.23560  0.40290  0.15260  0.2654  0.09438   \n",
      "135  ...  195.00  2227.0  0.12940  0.38850  0.47560  0.24320  0.2741  0.08574   \n",
      "143  ...  157.10  1748.0  0.15170  0.40020  0.42110  0.21340  0.3003  0.10480   \n",
      "145  ...  146.00  1479.0  0.16650  0.29420  0.53080  0.21730  0.3032  0.08075   \n",
      "147  ...  113.90   869.3  0.16130  0.35680  0.40690  0.18270  0.3179  0.10550   \n",
      "150  ...  145.40  1437.0  0.14010  0.37620  0.63990  0.19700  0.2972  0.09075   \n",
      "152  ...  139.80  1421.0  0.15280  0.18450  0.39770  0.14660  0.2293  0.06091   \n",
      "155  ...  153.90  1740.0  0.15140  0.37250  0.59360  0.20600  0.3266  0.09009   \n",
      "157  ...  143.40  1426.0  0.13090  0.23270  0.25440  0.14890  0.3251  0.07625   \n",
      "159  ...  163.20  1760.0  0.14640  0.35970  0.51790  0.21130  0.2480  0.08999   \n",
      "161  ...  232.20  3903.0  0.11540  0.17720  0.29170  0.17950  0.2336  0.06259   \n",
      "176  ...  142.60  1483.0  0.12870  0.24720  0.27530  0.13720  0.2404  0.07156   \n",
      "177  ...  128.00  1214.0  0.11940  0.20880  0.23850  0.13330  0.2652  0.07006   \n",
      "\n",
      "       33  34  \n",
      "174   2.4   2  \n",
      "175   3.5   0  \n",
      "178   1.2   0  \n",
      "179   2.0   2  \n",
      "180   0.8   0  \n",
      "181   2.5   0  \n",
      "182   2.0   4  \n",
      "183   1.0   0  \n",
      "184   2.5   0  \n",
      "185   2.0   0  \n",
      "186   4.0   0  \n",
      "187   1.0   1  \n",
      "188   3.0   7  \n",
      "189   2.1   2  \n",
      "190   4.0   0  \n",
      "191   0.8   1  \n",
      "192   1.7  21  \n",
      "193   6.0   2  \n",
      "194   1.5   0  \n",
      "195   3.7   0  \n",
      "197   3.5   0  \n",
      "37    1.8   0  \n",
      "39    2.5   0  \n",
      "40    3.2  13  \n",
      "42    1.5   0  \n",
      "43    1.2   1  \n",
      "46    3.0   2  \n",
      "48    3.0   1  \n",
      "50    4.0   2  \n",
      "56    3.0   1  \n",
      "59    3.5  15  \n",
      "62    2.5   9  \n",
      "67    9.0   7  \n",
      "69    3.5   0  \n",
      "75    4.0   4  \n",
      "81    1.2   4  \n",
      "88    1.5   0  \n",
      "105   6.0  15  \n",
      "109   1.8   1  \n",
      "110   4.0   1  \n",
      "112   8.5   6  \n",
      "114   2.5  11  \n",
      "120   2.0   0  \n",
      "125   2.5   2  \n",
      "128   3.5   4  \n",
      "129   1.5  13  \n",
      "131   4.5   0  \n",
      "132   2.6   0  \n",
      "135   3.0   1  \n",
      "143   0.4   0  \n",
      "145   4.5  27  \n",
      "147   3.5   4  \n",
      "150   1.5   7  \n",
      "152   2.0   7  \n",
      "155   7.0   9  \n",
      "157   3.5   2  \n",
      "159   5.5  20  \n",
      "161   3.0   4  \n",
      "176  10.0   9  \n",
      "177   2.7   4  \n",
      "\n",
      "[60 rows x 35 columns]\n",
      "     2      3      4       5       6        7        8        9        10  \\\n",
      "174  27  18.11  26.17  118.70   970.2  0.09867  0.17160  0.21770  0.09718   \n",
      "175  29  21.93  30.64  146.70  1487.0  0.08679  0.17230  0.20530  0.10100   \n",
      "178  24  24.29  25.48  161.80  1715.0  0.09374  0.22840  0.27020  0.13690   \n",
      "179  13  15.60  26.79   99.85   760.0  0.07885  0.05240  0.03778  0.02876   \n",
      "180  13  15.78  17.10  102.20   769.0  0.09668  0.09030  0.07268  0.04475   \n",
      "181  12  19.28  20.88  124.90  1127.0  0.09033  0.11970  0.06435  0.08870   \n",
      "182  15  15.66  24.51  102.00   771.1  0.08886  0.08731  0.09483  0.04286   \n",
      "183  11  22.44  27.42  150.60  1504.0  0.12110  0.20820  0.35790  0.18420   \n",
      "184  17  17.98  23.96  120.00   995.0  0.11570  0.17390  0.19540  0.12190   \n",
      "185  17  13.63  24.70   89.65   569.2  0.10550  0.13120  0.11610  0.06403   \n",
      "186   9  23.01  33.87  156.80  1705.0  0.11570  0.19480  0.29790  0.15220   \n",
      "187  10  22.41  29.95  145.50  1528.0  0.11190  0.16990  0.30760  0.15940   \n",
      "188  15  12.53  30.98   80.41   490.9  0.09252  0.06271  0.06151  0.03938   \n",
      "189   6  19.80  20.46  130.20  1235.0  0.09652  0.10770  0.15990  0.08705   \n",
      "190   8  19.96  27.41  130.80  1238.0  0.09075  0.11670  0.13550  0.08397   \n",
      "191   3  19.22  27.18  128.80  1134.0  0.10900  0.17770  0.21380  0.11160   \n",
      "192   3  14.72  25.26   99.28   657.5  0.11740  0.21120  0.17290  0.09465   \n",
      "193  10  22.52  21.92  146.90  1597.0  0.07592  0.09162  0.06862  0.06367   \n",
      "194   8  15.44  31.18  101.00   740.4  0.09399  0.10620  0.13750  0.06500   \n",
      "195  12  17.17  29.19  110.00   915.3  0.08952  0.06655  0.06583  0.05068   \n",
      "197   6  16.70  28.13  110.30   885.4  0.08896  0.11310  0.10120  0.04989   \n",
      "37    8  18.65  17.60  123.70  1076.0  0.10990  0.16860  0.19740  0.10090   \n",
      "39   48  15.10  22.02   97.26   712.8  0.09056  0.07081  0.05253  0.03334   \n",
      "40   11  19.21  18.57  125.50  1152.0  0.10530  0.12670  0.13230  0.08994   \n",
      "42   34  14.25  22.15   96.42   645.7  0.10490  0.20080  0.21350  0.08653   \n",
      "43   19  12.68  23.84   82.69   499.0  0.11220  0.12620  0.11280  0.06873   \n",
      "46   26  13.80  15.79   90.43   584.1  0.10070  0.12800  0.07789  0.05069   \n",
      "48   40  19.10  26.29  129.10  1132.0  0.12150  0.17910  0.19370  0.14690   \n",
      "50   10  20.77  22.83  137.40  1336.0  0.10330  0.15150  0.16370  0.10150   \n",
      "56   35  17.95  20.01  114.20   982.0  0.08402  0.06722  0.07293  0.05596   \n",
      "59   73  19.00  18.91  123.40  1138.0  0.08217  0.08028  0.09271  0.05627   \n",
      "62    7  20.29  21.49  132.80  1291.0  0.09258  0.12050  0.15230  0.08636   \n",
      "67   44  17.68  20.74  117.40   963.7  0.11150  0.16650  0.18550  0.10540   \n",
      "69    8  19.59  18.15  130.70  1214.0  0.11200  0.16660  0.25080  0.12860   \n",
      "75   17  27.22  21.87  182.10  2250.0  0.10940  0.19140  0.28710  0.18780   \n",
      "81   74  12.34  26.86   81.15   477.4  0.10340  0.13530  0.10850  0.04562   \n",
      "88   17  15.12  16.68   98.78   716.6  0.08876  0.09588  0.07550  0.04079   \n",
      "105  19  19.55  28.77  133.60  1207.0  0.09260  0.20630  0.17840  0.11440   \n",
      "109  39  20.31  27.06  132.90  1288.0  0.10000  0.10880  0.15190  0.09333   \n",
      "110  12  17.29  22.13  114.40   947.8  0.08999  0.12730  0.09697  0.07507   \n",
      "112  37  17.19  22.07  111.60   928.3  0.09726  0.08995  0.09061  0.06527   \n",
      "114   5  21.75  20.99  147.30  1491.0  0.09401  0.19610  0.21950  0.10880   \n",
      "120  58  19.53  18.90  129.50  1217.0  0.11500  0.16420  0.21970  0.10620   \n",
      "125  78  16.27  20.71  106.90   813.7  0.11690  0.13190  0.14780  0.08488   \n",
      "128  24  23.51  24.27  155.10  1747.0  0.10690  0.12830  0.23080  0.14100   \n",
      "129  33  19.68  21.68  129.90  1194.0  0.09797  0.13390  0.18630  0.11030   \n",
      "131   4  25.73  17.46  174.20  2010.0  0.11490  0.23630  0.33680  0.19130   \n",
      "132   5  15.08  25.74   98.00   716.6  0.10240  0.09769  0.12350  0.06553   \n",
      "135   2  22.01  21.90  147.20  1482.0  0.10630  0.19540  0.24480  0.15010   \n",
      "143  49  15.50  21.08  102.90   803.1  0.11200  0.15710  0.15220  0.08481   \n",
      "145   1  20.18  19.54  133.80  1250.0  0.11330  0.14890  0.21330  0.12590   \n",
      "147   9  13.98  19.62   91.12   599.5  0.10600  0.11330  0.11260  0.06463   \n",
      "150  16  17.75  28.03  117.30   981.6  0.09997  0.13140  0.16980  0.08293   \n",
      "152   9  19.59  25.00  127.70  1191.0  0.10320  0.09871  0.16550  0.09063   \n",
      "155  14  19.44  18.82  128.10  1167.0  0.10890  0.14480  0.22560  0.11940   \n",
      "157  12  18.01  20.56  118.40  1007.0  0.10010  0.12890  0.11700  0.07762   \n",
      "159  11  20.59  21.24  137.80  1320.0  0.10850  0.16440  0.21880  0.11210   \n",
      "161   7  24.24  18.74  159.60  1945.0  0.08938  0.11360  0.17270  0.10710   \n",
      "176  14  17.53  25.28  114.00   966.6  0.09278  0.09175  0.11050  0.06741   \n",
      "177   1  18.11  30.99  115.80   984.4  0.08625  0.09240  0.06214  0.05598   \n",
      "\n",
      "         11  ...      25      26       27       28       29       30      31  \\\n",
      "174  0.2063  ...  145.10  1251.0  0.14640  0.51270  0.87740  0.26030  0.3941   \n",
      "175  0.1796  ...  171.50  1951.0  0.11680  0.40720  0.44940  0.18860  0.2784   \n",
      "178  0.2307  ...  184.80  2213.0  0.12470  0.39350  0.61180  0.20630  0.3983   \n",
      "179  0.1580  ...  117.20  1027.0  0.10970  0.09745  0.12630  0.08222  0.2805   \n",
      "180  0.1890  ...  111.70   921.4  0.13860  0.26400  0.29010  0.12340  0.3261   \n",
      "181  0.1971  ...  130.00  1214.0  0.09273  0.12590  0.08201  0.11160  0.2147   \n",
      "182  0.1995  ...  124.00  1066.0  0.11820  0.24860  0.35080  0.12730  0.2964   \n",
      "183  0.2524  ...  161.20  1689.0  0.13300  0.25800  0.47410  0.21350  0.2809   \n",
      "184  0.1981  ...  135.20  1225.0  0.13400  0.26060  0.31240  0.17530  0.2746   \n",
      "185  0.1791  ...  107.70   712.2  0.15540  0.35150  0.34090  0.16890  0.2739   \n",
      "186  0.1799  ...  197.80  2481.0  0.14320  0.28950  0.49370  0.21100  0.2774   \n",
      "187  0.2099  ...  194.50  2690.0  0.12510  0.38320  0.58420  0.24220  0.2729   \n",
      "188  0.1993  ...   96.75   694.6  0.15600  0.13580  0.24360  0.12590  0.3292   \n",
      "189  0.1620  ...  155.10  1623.0  0.13730  0.23680  0.39310  0.18400  0.2382   \n",
      "190  0.1600  ...  150.30  1603.0  0.12930  0.25130  0.41060  0.16380  0.2245   \n",
      "191  0.1924  ...  161.00  1565.0  0.15010  0.51300  0.68410  0.20990  0.3484   \n",
      "192  0.2079  ...  111.60   814.8  0.14640  0.53520  0.56550  0.19740  0.3778   \n",
      "193  0.1728  ...  162.10  1902.0  0.08191  0.13190  0.10560  0.09378  0.2061   \n",
      "194  0.1735  ...  112.60   929.0  0.12720  0.23620  0.29750  0.12860  0.2914   \n",
      "195  0.1793  ...  132.50  1295.0  0.12610  0.15720  0.21410  0.09520  0.3362   \n",
      "197  0.1890  ...  128.80  1213.0  0.13300  0.28080  0.34550  0.13170  0.3035   \n",
      "37   0.1907  ...  150.60  1567.0  0.16790  0.50900  0.73450  0.23780  0.3799   \n",
      "39   0.1616  ...  117.70  1030.0  0.13890  0.20570  0.27120  0.15300  0.2675   \n",
      "40   0.1917  ...  170.10  2145.0  0.16240  0.35110  0.38790  0.20910  0.3537   \n",
      "42   0.1949  ...  119.10   959.5  0.16400  0.62470  0.69220  0.17850  0.2844   \n",
      "43   0.1905  ...  111.80   888.3  0.18510  0.40610  0.40240  0.17160  0.3383   \n",
      "46   0.1662  ...  110.30   812.4  0.14110  0.35420  0.27790  0.13830  0.2589   \n",
      "48   0.1634  ...  141.30  1298.0  0.13920  0.28170  0.24320  0.18410  0.2311   \n",
      "50   0.1800  ...  159.90  1816.0  0.13850  0.41070  0.37570  0.18810  0.3371   \n",
      "56   0.2129  ...  129.20  1261.0  0.10720  0.12020  0.22490  0.11850  0.4882   \n",
      "59   0.1946  ...  148.20  1538.0  0.10210  0.22640  0.32070  0.12180  0.2841   \n",
      "62   0.2356  ...  171.60  2196.0  0.12250  0.34490  0.45090  0.19430  0.4416   \n",
      "67   0.1971  ...  132.90  1302.0  0.14180  0.34980  0.35830  0.15150  0.2463   \n",
      "69   0.2027  ...  174.90  2232.0  0.14380  0.38460  0.68100  0.22470  0.3643   \n",
      "75   0.1800  ...  220.80  3216.0  0.14720  0.40340  0.53400  0.26880  0.2856   \n",
      "81   0.1943  ...  101.70   768.9  0.17850  0.47060  0.44250  0.14590  0.3215   \n",
      "88   0.1594  ...  117.70   989.5  0.14910  0.33310  0.33270  0.12520  0.3415   \n",
      "105  0.1893  ...  178.60  1926.0  0.12810  0.53290  0.42510  0.19410  0.2818   \n",
      "109  0.1814  ...  162.30  1844.0  0.15220  0.29450  0.37880  0.16970  0.3151   \n",
      "110  0.2108  ...  137.90  1295.0  0.11340  0.28670  0.22980  0.15280  0.3067   \n",
      "112  0.1867  ...  140.50  1436.0  0.15580  0.25670  0.38890  0.19840  0.3216   \n",
      "114  0.1721  ...  195.90  2384.0  0.12720  0.47250  0.58070  0.18410  0.2833   \n",
      "120  0.1792  ...  171.10  2053.0  0.14950  0.41160  0.61210  0.19800  0.2968   \n",
      "125  0.1948  ...  129.80  1121.0  0.15900  0.29470  0.35970  0.15830  0.3103   \n",
      "128  0.1797  ...  202.40  2906.0  0.15150  0.26780  0.48190  0.20890  0.2593   \n",
      "129  0.2082  ...  157.60  1540.0  0.12180  0.34580  0.47340  0.22550  0.4045   \n",
      "131  0.1956  ...  229.30  3234.0  0.15300  0.59370  0.64510  0.27560  0.3690   \n",
      "132  0.1647  ...  121.20  1050.0  0.16600  0.23560  0.40290  0.15260  0.2654   \n",
      "135  0.1824  ...  195.00  2227.0  0.12940  0.38850  0.47560  0.24320  0.2741   \n",
      "143  0.2085  ...  157.10  1748.0  0.15170  0.40020  0.42110  0.21340  0.3003   \n",
      "145  0.1724  ...  146.00  1479.0  0.16650  0.29420  0.53080  0.21730  0.3032   \n",
      "147  0.1669  ...  113.90   869.3  0.16130  0.35680  0.40690  0.18270  0.3179   \n",
      "150  0.1713  ...  145.40  1437.0  0.14010  0.37620  0.63990  0.19700  0.2972   \n",
      "152  0.1663  ...  139.80  1421.0  0.15280  0.18450  0.39770  0.14660  0.2293   \n",
      "155  0.1823  ...  153.90  1740.0  0.15140  0.37250  0.59360  0.20600  0.3266   \n",
      "157  0.2116  ...  143.40  1426.0  0.13090  0.23270  0.25440  0.14890  0.3251   \n",
      "159  0.1848  ...  163.20  1760.0  0.14640  0.35970  0.51790  0.21130  0.2480   \n",
      "161  0.1591  ...  232.20  3903.0  0.11540  0.17720  0.29170  0.17950  0.2336   \n",
      "176  0.1424  ...  142.60  1483.0  0.12870  0.24720  0.27530  0.13720  0.2404   \n",
      "177  0.1603  ...  128.00  1214.0  0.11940  0.20880  0.23850  0.13330  0.2652   \n",
      "\n",
      "          32    33  34  \n",
      "174  0.09318   2.4   2  \n",
      "175  0.07353   3.5   0  \n",
      "178  0.07978   1.2   0  \n",
      "179  0.06378   2.0   2  \n",
      "180  0.07752   0.8   0  \n",
      "181  0.05644   2.5   0  \n",
      "182  0.06591   2.0   4  \n",
      "183  0.06318   1.0   0  \n",
      "184  0.07830   2.5   0  \n",
      "185  0.09945   2.0   0  \n",
      "186  0.07542   4.0   0  \n",
      "187  0.09391   1.0   1  \n",
      "188  0.09406   3.0   7  \n",
      "189  0.06831   2.1   2  \n",
      "190  0.07842   4.0   0  \n",
      "191  0.09774   0.8   1  \n",
      "192  0.11320   1.7  21  \n",
      "193  0.05788   6.0   2  \n",
      "194  0.08024   1.5   0  \n",
      "195  0.06033   3.7   0  \n",
      "197  0.08036   3.5   0  \n",
      "37   0.09185   1.8   0  \n",
      "39   0.07873   2.5   0  \n",
      "40   0.08294   3.2  13  \n",
      "42   0.11320   1.5   0  \n",
      "43   0.10310   1.2   1  \n",
      "46   0.10300   3.0   2  \n",
      "48   0.09203   3.0   1  \n",
      "50   0.07651   4.0   2  \n",
      "56   0.06111   3.0   1  \n",
      "59   0.06541   3.5  15  \n",
      "62   0.07863   2.5   9  \n",
      "67   0.07738   9.0   7  \n",
      "69   0.09223   3.5   0  \n",
      "75   0.08082   4.0   4  \n",
      "81   0.12050   1.2   4  \n",
      "88   0.09740   1.5   0  \n",
      "105  0.10050   6.0  15  \n",
      "109  0.07999   1.8   1  \n",
      "110  0.07484   4.0   1  \n",
      "112  0.07570   8.5   6  \n",
      "114  0.08858   2.5  11  \n",
      "120  0.09929   2.0   0  \n",
      "125  0.08200   2.5   2  \n",
      "128  0.07738   3.5   4  \n",
      "129  0.07918   1.5  13  \n",
      "131  0.08815   4.5   0  \n",
      "132  0.09438   2.6   0  \n",
      "135  0.08574   3.0   1  \n",
      "143  0.10480   0.4   0  \n",
      "145  0.08075   4.5  27  \n",
      "147  0.10550   3.5   4  \n",
      "150  0.09075   1.5   7  \n",
      "152  0.06091   2.0   7  \n",
      "155  0.09009   7.0   9  \n",
      "157  0.07625   3.5   2  \n",
      "159  0.08999   5.5  20  \n",
      "161  0.06259   3.0   4  \n",
      "176  0.07156  10.0   9  \n",
      "177  0.07006   2.7   4  \n",
      "\n",
      "[60 rows x 33 columns] 174    N\n",
      "175    N\n",
      "178    N\n",
      "179    N\n",
      "180    N\n",
      "181    N\n",
      "182    N\n",
      "183    N\n",
      "184    N\n",
      "185    N\n",
      "186    N\n",
      "187    N\n",
      "188    N\n",
      "189    N\n",
      "190    N\n",
      "191    N\n",
      "192    N\n",
      "193    N\n",
      "194    N\n",
      "195    N\n",
      "197    N\n",
      "37     R\n",
      "39     R\n",
      "40     R\n",
      "42     R\n",
      "43     R\n",
      "46     R\n",
      "48     R\n",
      "50     R\n",
      "56     R\n",
      "59     R\n",
      "62     R\n",
      "67     R\n",
      "69     R\n",
      "75     R\n",
      "81     R\n",
      "88     R\n",
      "105    R\n",
      "109    R\n",
      "110    R\n",
      "112    R\n",
      "114    R\n",
      "120    R\n",
      "125    R\n",
      "128    R\n",
      "129    R\n",
      "131    R\n",
      "132    R\n",
      "135    R\n",
      "143    R\n",
      "145    R\n",
      "147    R\n",
      "150    R\n",
      "152    R\n",
      "155    R\n",
      "157    R\n",
      "159    R\n",
      "161    R\n",
      "176    R\n",
      "177    R\n",
      "Name: 1, dtype: object\n",
      "      2      3      4       5       6        7        8       9        10  \\\n",
      "0     31  18.02  27.60  117.50  1013.0  0.09489  0.10360  0.1086  0.07055   \n",
      "1     61  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710   \n",
      "2    116  21.37  17.44  137.50  1373.0  0.08836  0.11890  0.1255  0.08180   \n",
      "3    123  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520   \n",
      "6     60  18.98  19.61  124.40  1112.0  0.09087  0.12370  0.1213  0.08910   \n",
      "..   ...    ...    ...     ...     ...      ...      ...     ...      ...   \n",
      "132    5  15.08  25.74   98.00   716.6  0.10240  0.09769  0.1235  0.06553   \n",
      "135    2  22.01  21.90  147.20  1482.0  0.10630  0.19540  0.2448  0.15010   \n",
      "143   49  15.50  21.08  102.90   803.1  0.11200  0.15710  0.1522  0.08481   \n",
      "145    1  20.18  19.54  133.80  1250.0  0.11330  0.14890  0.2133  0.12590   \n",
      "196    3  21.42  22.84  145.00  1440.0  0.10700  0.19390  0.2380  0.13180   \n",
      "\n",
      "         11  ...      25      26      27      28      29      30      31  \\\n",
      "0    0.1865  ...  139.70  1436.0  0.1195  0.1926  0.3140  0.1170  0.2677   \n",
      "1    0.2419  ...  184.60  2019.0  0.1622  0.6656  0.7119  0.2654  0.4601   \n",
      "2    0.2333  ...  159.10  1949.0  0.1188  0.3449  0.3414  0.2032  0.4334   \n",
      "3    0.2597  ...   98.87   567.7  0.2098  0.8663  0.6869  0.2575  0.6638   \n",
      "6    0.1727  ...  152.60  1593.0  0.1144  0.3371  0.2990  0.1922  0.2726   \n",
      "..      ...  ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "132  0.1647  ...  121.20  1050.0  0.1660  0.2356  0.4029  0.1526  0.2654   \n",
      "135  0.1824  ...  195.00  2227.0  0.1294  0.3885  0.4756  0.2432  0.2741   \n",
      "143  0.2085  ...  157.10  1748.0  0.1517  0.4002  0.4211  0.2134  0.3003   \n",
      "145  0.1724  ...  146.00  1479.0  0.1665  0.2942  0.5308  0.2173  0.3032   \n",
      "196  0.1884  ...  198.30  2375.0  0.1498  0.4379  0.5411  0.2215  0.2832   \n",
      "\n",
      "          32   33  34  \n",
      "0    0.08113  5.0   5  \n",
      "1    0.11890  3.0   2  \n",
      "2    0.09067  2.5   0  \n",
      "3    0.17300  2.0   0  \n",
      "6    0.09581  1.5   ?  \n",
      "..       ...  ...  ..  \n",
      "132  0.09438  2.6   0  \n",
      "135  0.08574  3.0   1  \n",
      "143  0.10480  0.4   0  \n",
      "145  0.08075  4.5  27  \n",
      "196  0.08981  3.0   ?  \n",
      "\n",
      "[168 rows x 33 columns] 0      N\n",
      "1      N\n",
      "2      N\n",
      "3      N\n",
      "6      N\n",
      "      ..\n",
      "132    R\n",
      "135    R\n",
      "143    R\n",
      "145    R\n",
      "196    R\n",
      "Name: 1, Length: 168, dtype: object\n"
     ]
    }
   ],
   "source": [
    "non_rec = data2[data2[1] == 'N']\n",
    "\n",
    "train_non_rec = non_rec[0:130]\n",
    "test_non_rec = non_rec[130:]\n",
    "\n",
    "\n",
    "rec = data2[data2[1] == 'R']\n",
    "\n",
    "rec_last = rec.loc[196:197,0:34]\n",
    "\n",
    "train_rec = rec[0:37]\n",
    "test_rec = rec.loc[37:177, 0:34]\n",
    "print(test_rec)\n",
    "\n",
    "test2 = pd.concat([test_non_rec, test_rec])\n",
    "print('hello',test2)\n",
    "x_test = test2.iloc[:, 2:]\n",
    "y_test = test2.iloc[:, 1]\n",
    "print(x_test, y_test)\n",
    "train2 = pd.concat([train_non_rec, train_rec, rec_last])\n",
    "x_train = train2.iloc[:, 2:]\n",
    "y_train = train2.iloc[:, 1]\n",
    "print(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.c \n",
    "\n",
    "There are four instances in your training set that are missing the lymph node feature (denoted as ?). This is not a very severe issue, so replace the missing features with the median of the lymph node feature in your training set. (10 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.100e+01, 1.802e+01, 2.760e+01, ..., 8.113e-02, 5.000e+00,\n",
       "        5.000e+00],\n",
       "       [6.100e+01, 1.799e+01, 1.038e+01, ..., 1.189e-01, 3.000e+00,\n",
       "        2.000e+00],\n",
       "       [1.160e+02, 2.137e+01, 1.744e+01, ..., 9.067e-02, 2.500e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [4.900e+01, 1.550e+01, 2.108e+01, ..., 1.048e-01, 4.000e-01,\n",
       "        0.000e+00],\n",
       "       [1.000e+00, 2.018e+01, 1.954e+01, ..., 8.075e-02, 4.500e+00,\n",
       "        2.700e+01],\n",
       "       [3.000e+00, 2.142e+01, 2.284e+01, ..., 8.981e-02, 3.000e+00,\n",
       "        1.000e+00]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "# convert to DF\n",
    "x_train = pd.DataFrame(x_train)\n",
    "y_train = pd.DataFrame(y_train)\n",
    "x_train = x_train.apply(pd.to_numeric, errors = 'coerce')\n",
    "\n",
    "\n",
    "mean = SimpleImputer(missing_values = np.nan, strategy = 'median')\n",
    "mean.fit(x_train)\n",
    "x_train = mean.transform(x_train)\n",
    "x_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.d(i) Binary Classification Using Naive Bayes' Classifiers\n",
    "\n",
    "Solve the problem using a Naive Bayes' classifier. Use Gaussian class conditional distributions. Report the confusion matrix, ROC, precision, recall, F1 score, and AUC for both the train and test data sets. (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC \n",
      " 0.9875\n",
      "Train AUC \n",
      " 0.9529652351738241\n",
      "Test Roc_Curve\n",
      " [0.         0.03333333 1.        ] [0. 1. 1.]\n",
      "Train Roc_Curve\n",
      " [0.         0.97068404 1.        ] [0.         0.97068404 1.        ]\n",
      "Test CF Matrix\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98        30\n",
      "           1       0.98      1.00      0.99        50\n",
      "\n",
      "    accuracy                           0.99        80\n",
      "   macro avg       0.99      0.98      0.99        80\n",
      "weighted avg       0.99      0.99      0.99        80\n",
      "\n",
      "Train CF Matrix\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94       182\n",
      "           1       0.96      0.97      0.96       307\n",
      "\n",
      "    accuracy                           0.95       489\n",
      "   macro avg       0.95      0.95      0.95       489\n",
      "weighted avg       0.95      0.95      0.95       489\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train, y_train)\n",
    "pred_test = nb.predict(x_test)\n",
    "pred_train = nb.predict(x_train)\n",
    "\n",
    "\n",
    "pred_test = log.predict(x_test)\n",
    "pred_train = log.predict(x_train)\n",
    "print('Test AUC \\n', metrics.accuracy_score(y_test, pred_test))\n",
    "print('Train AUC \\n', metrics.accuracy_score(y_train, pred_train))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_test.reshape(-1,1), pos_label = 1)\n",
    "\n",
    "fpr1, tpr1, _ = roc_curve(y_train, pred_train.reshape(-1,1) , pos_label = 1)\n",
    "print('Test Roc_Curve\\n', fpr, tpr)\n",
    "print('Train Roc_Curve\\n', tpr1, tpr1)\n",
    "cf_matrix=confusion_matrix(y_test,pred_test)\n",
    "cf_matrix1=confusion_matrix(y_train,pred_train)\n",
    "\n",
    "print('Test CF Matrix\\n',classification_report(y_test,pred_test) )\n",
    "print('Train CF Matrix\\n', classification_report(y_train,pred_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.8.0-py3-none-any.whl (206 kB)\n",
      "\u001b[K     |████████████████████████████████| 206 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /Users/tara/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (0.17.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/tara/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.5.2)\n",
      "Collecting scikit-learn>=0.24\n",
      "  Downloading scikit_learn-0.24.1-cp38-cp38-macosx_10_13_x86_64.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 50.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /Users/tara/opt/anaconda3/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/tara/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n",
      "Installing collected packages: scikit-learn, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 0.23.2\n",
      "    Uninstalling scikit-learn-0.23.2:\n",
      "      Successfully uninstalled scikit-learn-0.23.2\n",
      "Successfully installed imbalanced-learn-0.8.0 imblearn-0.0 scikit-learn-0.24.1\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.d(ii)\n",
    "\n",
    "This data set is rather imbalanced. Balance your data set using SMOTE,\n",
    "by downsampling the common class in the training set to 90 instances and\n",
    "upsampling the uncommon class to 90 instances. Use k = 5 nearest neighbors in SMOTE. Remember not to change the balance of the test set. Report the confusion matrix, ROC, precision, recall, F1 score, and AUC for both the train and test data sets. Does SMOTE help? (15 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'delayed' from 'sklearn.utils.fixes' (/Users/tara/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6542fe5c49b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# smote on sklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munder_sampling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mover_sampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcombine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munder_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEditedNearestNeighbours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/over_sampling/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_adasyn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mADASYN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_random_over_sampler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBorderlineSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeansSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/over_sampling/_smote/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTENC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeansSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBorderlineSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/imblearn/over_sampling/_smote/cluster.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/cluster/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spectral\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_clustering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpectralClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m from ._mean_shift import (mean_shift, MeanShift,\n\u001b[1;32m      8\u001b[0m                           estimate_bandwidth, get_bin_seeds)\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/cluster/_spectral.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpairwise_kernels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanifold\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_kmeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mk_means\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_locally_linear\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocally_linear_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocallyLinearEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_isomap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIsomap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_mds\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmacof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_spectral_embedding\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpectralEmbedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectral_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_t_sne\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrustworthiness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/manifold/_mds.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'delayed' from 'sklearn.utils.fixes' (/Users/tara/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py)"
     ]
    }
   ],
   "source": [
    "# smote on sklearn\n",
    "from imblearn import under_sampling, over_sampling\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.d(iii) \n",
    "\n",
    "(Extra Credit, 10 points) Repeat 2(d)i and 2(d)ii using multinomial priors.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
